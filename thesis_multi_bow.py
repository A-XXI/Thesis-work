# -*- coding: utf-8 -*-
"""Thesis_M_B.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yzhZW22o6KPMsS3IF9TPW3PsadnjPqHJ
"""

import numpy as np
import pandas as pd

from google.colab import drive
drive.mount('/content/gdrive')
path="/content/gdrive/My Drive/Thesis/Data - Sheet1.csv"
data = pd.read_csv(path)

data.head()

x = data.drop(['Hate','Type'], axis=1)
y = data['Type']

x.head()
y.head()

y = y.fillna("Not Hate")

y.isnull().sum()

y[8]

texts=x.copy()

texts['Comment'][2913]

import re
# Cleaning the texts
corpus = []
for i in range(0,len(texts)):
    review = re.sub('[^a-zA-Z]', ' ',str(texts['Comment'][i]))
    review = review.lower()
    review = review.split()
    review = ' '.join(review)
    corpus.append(review)

#Checking cleaned Text
corpus[2913]

#CounterVectorizer
#bag of words
from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features=8000,ngram_range=(1,2))
X = cv.fit_transform(corpus).toarray()

import matplotlib.pyplot as plt
def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):
    """
    See full source and example: 
    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html
    
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

y.unique()

## Divide the dataset into Train and Test
from sklearn.model_selection import train_test_split
from collections import Counter
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"Training target statistics: {Counter(y_train)}")
print(f"Testing target statistics: {Counter(y_test)}")

import sklearn.metrics as metrics
import itertools

from sklearn.linear_model import LogisticRegression
lgc=LogisticRegression(max_iter=7000)
lgc.fit(X_train,y_train)
pred = lgc.predict(X_test)
score = metrics.accuracy_score(y_test, pred)
print("accuracy:   %0.3f" % score)
from sklearn.metrics import precision_recall_fscore_support as score
precision, recall, fscore, support = score(y_test, pred, average='weighted')
print('precision: {}'.format(precision))
print('recall: {}'.format(recall))
print('fscore: {}'.format(fscore))
print('support: {}'.format(support))

cm = metrics.confusion_matrix(y_test, pred)
plot_confusion_matrix(cm, classes=['Appearance', 'Not Hate', 'Others', 'Racial', 'Religious', 'Sexual', 'Slang'])

### MultinomialNB Algorithm

from sklearn.naive_bayes import MultinomialNB
MNB=MultinomialNB()
MNB.fit(X_train, y_train)
pred = MNB.predict(X_test)
score = metrics.accuracy_score(y_test, pred)
print("accuracy:   %0.2f" % score)
from sklearn.metrics import precision_recall_fscore_support as score
precision, recall, fscore, support = score(y_test, pred, average='weighted')
print('precision: {}'.format(precision))
print('recall: {}'.format(recall))
print('fscore: {}'.format(fscore))
print('support: {}'.format(support))

cm = metrics.confusion_matrix(y_test, pred)
plot_confusion_matrix(cm, classes=['Appearance', 'Not Hate', 'Others', 'Racial', 'Religious', 'Sexual', 'Slang'])

### GaussianNB Algorithm

from sklearn.naive_bayes import GaussianNB
GNB = GaussianNB()
GNB.fit(X_train, y_train)
pred = GNB.predict(X_test)
score = metrics.accuracy_score(y_test, pred)
print("accuracy:   %0.2f" % score)
from sklearn.metrics import precision_recall_fscore_support as score
precision, recall, fscore, support = score(y_test, pred, average='weighted')
print('precision: {}'.format(precision))
print('recall: {}'.format(recall))
print('fscore: {}'.format(fscore))
print('support: {}'.format(support))

cm = metrics.confusion_matrix(y_test, pred)
plot_confusion_matrix(cm, classes=['Appearance', 'Not Hate', 'Others', 'Racial', 'Religious', 'Sexual', 'Slang'])

### DecisionTreeClassifier

from sklearn.tree import DecisionTreeClassifier
dc=DecisionTreeClassifier(max_depth=3)
dc.fit(X_train, y_train)
pred = dc.predict(X_test)
score = metrics.accuracy_score(y_test, pred)
print("accuracy:   %0.2f" % score)
from sklearn.metrics import precision_recall_fscore_support as score
precision, recall, fscore, support = score(y_test, pred, average='weighted')
print('precision: {}'.format(precision))
print('recall: {}'.format(recall))
print('fscore: {}'.format(fscore))
print('support: {}'.format(support))

cm = metrics.confusion_matrix(y_test, pred)
plot_confusion_matrix(cm, classes=['Appearance', 'Not Hate', 'Others', 'Racial', 'Religious', 'Sexual', 'Slang'])

### AdaBoostClassifier
from sklearn.ensemble import AdaBoostClassifier
ada= AdaBoostClassifier()
ada.fit(X_train, y_train)
pred = ada.predict(X_test)
score = metrics.accuracy_score(y_test, pred)
print("accuracy:   %0.3f" % score)
from sklearn.metrics import precision_recall_fscore_support as score
precision, recall, fscore, support = score(y_test, pred, average='weighted')
print('precision: {}'.format(precision))
print('recall: {}'.format(recall))
print('fscore: {}'.format(fscore))
print('support: {}'.format(support))

cm = metrics.confusion_matrix(y_test, pred)
plot_confusion_matrix(cm, classes=['Appearance', 'Not Hate', 'Others', 'Racial', 'Religious', 'Sexual', 'Slang'])

### RandomForestClassifier

from sklearn.ensemble import RandomForestClassifier
rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)
pred = rf_model.predict(X_test)
score = metrics.accuracy_score(y_test, pred)
print("accuracy:   %0.3f" % score)
from sklearn.metrics import precision_recall_fscore_support as score
precision, recall, fscore, support = score(y_test, pred, average='weighted')
print('precision: {}'.format(precision))
print('recall: {}'.format(recall))
print('fscore: {}'.format(fscore))
print('support: {}'.format(support))

cm = metrics.confusion_matrix(y_test, pred)
plot_confusion_matrix(cm, classes=['Appearance', 'Not Hate', 'Others', 'Racial', 'Religious', 'Sexual', 'Slang'])

from sklearn.svm import SVC
svc = SVC(C=1,kernel='linear',gamma = 'auto',probability=True)
svc.fit(X_train, y_train)
pred = svc.predict(X_test)
score = metrics.accuracy_score(y_test, pred)
print("accuracy:   %0.3f" % score)
from sklearn.metrics import precision_recall_fscore_support as score
precision, recall, fscore, support = score(y_test, pred, average='weighted')
print('precision: {}'.format(precision))
print('recall: {}'.format(recall))
print('fscore: {}'.format(fscore))
print('support: {}'.format(support))

cm = metrics.confusion_matrix(y_test, pred)
plot_confusion_matrix(cm, classes=['Appearance', 'Not Hate', 'Others', 'Racial', 'Religious', 'Sexual', 'Slang'])

!pip install -q wordcloud
import wordcloud
import sklearn.metrics as metrics
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger') 
import matplotlib.pyplot as plt
import io
import unicodedata
import string
import re
import itertools
from wordcloud import WordCloud